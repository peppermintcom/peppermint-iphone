// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/cloud/vision/v1/image_annotator.proto

#import "GPBProtocolBuffers.h"

#if GOOGLE_PROTOBUF_OBJC_GEN_VERSION != 30000
#error This file was generated by a different version of protoc which is incompatible with your Protocol Buffer library sources.
#endif

// @@protoc_insertion_point(imports)

CF_EXTERN_C_BEGIN

@class BoundingPoly;
@class Color;
@class DominantColorsAnnotation;
@class Image;
@class ImageContext;
@class ImageProperties;
@class ImageSource;
@class LatLng;
@class LatLongRect;
@class Position;
@class SafeSearchAnnotation;
@class Status;

NS_ASSUME_NONNULL_BEGIN

#pragma mark - Enum Likelihood

// A bucketized representation of likelihood meant to give our clients highly
// stable results across model upgrades.
typedef GPB_ENUM(Likelihood) {
  Likelihood_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  // Unknown likelihood.
  Likelihood_Unknown = 0,

  // The image very unlikely belongs to the vertical specified.
  Likelihood_VeryUnlikely = 1,

  // The image unlikely belongs to the vertical specified.
  Likelihood_Unlikely = 2,

  // The image possibly belongs to the vertical specified.
  Likelihood_Possible = 3,

  // The image likely belongs to the vertical specified.
  Likelihood_Likely = 4,

  // The image very likely belongs to the vertical specified.
  Likelihood_VeryLikely = 5,
};

GPBEnumDescriptor *Likelihood_EnumDescriptor(void);

BOOL Likelihood_IsValidValue(int32_t value);

#pragma mark - Enum Feature_Type

// Type of image feature.
typedef GPB_ENUM(Feature_Type) {
  Feature_Type_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  // Unspecified feature type.
  Feature_Type_TypeUnspecified = 0,

  // Run face detection.
  Feature_Type_FaceDetection = 1,

  // Run landmark detection.
  Feature_Type_LandmarkDetection = 2,

  // Run logo detection.
  Feature_Type_LogoDetection = 3,

  // Run label detection.
  Feature_Type_LabelDetection = 4,

  // Run OCR.
  Feature_Type_TextDetection = 5,

  // Run various computer vision models to
  Feature_Type_SafeSearchDetection = 6,

  // compute image safe-search properties.
  Feature_Type_ImageProperties = 7,
};

GPBEnumDescriptor *Feature_Type_EnumDescriptor(void);

BOOL Feature_Type_IsValidValue(int32_t value);

#pragma mark - Enum FaceAnnotation_Landmark_Type

// Face landmark (feature) type.
// Left and right are defined from the vantage of the viewer of the image,
// without considering mirror projections typical of photos. So, LEFT_EYE,
// typically is the person's right eye.
typedef GPB_ENUM(FaceAnnotation_Landmark_Type) {
  FaceAnnotation_Landmark_Type_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  // Unknown face landmark detected. Should not be filled.
  FaceAnnotation_Landmark_Type_UnknownLandmark = 0,

  // Left eye.
  FaceAnnotation_Landmark_Type_LeftEye = 1,

  // Right eye.
  FaceAnnotation_Landmark_Type_RightEye = 2,

  // Left of left eyebrow.
  FaceAnnotation_Landmark_Type_LeftOfLeftEyebrow = 3,

  // Right of left eyebrow.
  FaceAnnotation_Landmark_Type_RightOfLeftEyebrow = 4,

  // Left of right eyebrow.
  FaceAnnotation_Landmark_Type_LeftOfRightEyebrow = 5,

  // Right of right eyebrow.
  FaceAnnotation_Landmark_Type_RightOfRightEyebrow = 6,

  // Midpoint between eyes.
  FaceAnnotation_Landmark_Type_MidpointBetweenEyes = 7,

  // Nose tip.
  FaceAnnotation_Landmark_Type_NoseTip = 8,

  // Upper lip.
  FaceAnnotation_Landmark_Type_UpperLip = 9,

  // Lower lip.
  FaceAnnotation_Landmark_Type_LowerLip = 10,

  // Mouth left.
  FaceAnnotation_Landmark_Type_MouthLeft = 11,

  // Mouth right.
  FaceAnnotation_Landmark_Type_MouthRight = 12,

  // Mouth center.
  FaceAnnotation_Landmark_Type_MouthCenter = 13,

  // Nose, bottom right.
  FaceAnnotation_Landmark_Type_NoseBottomRight = 14,

  // Nose, bottom left.
  FaceAnnotation_Landmark_Type_NoseBottomLeft = 15,

  // Nose, bottom center.
  FaceAnnotation_Landmark_Type_NoseBottomCenter = 16,

  // Left eye, top boundary.
  FaceAnnotation_Landmark_Type_LeftEyeTopBoundary = 17,

  // Left eye, right corner.
  FaceAnnotation_Landmark_Type_LeftEyeRightCorner = 18,

  // Left eye, bottom boundary.
  FaceAnnotation_Landmark_Type_LeftEyeBottomBoundary = 19,

  // Left eye, left corner.
  FaceAnnotation_Landmark_Type_LeftEyeLeftCorner = 20,

  // Right eye, top boundary.
  FaceAnnotation_Landmark_Type_RightEyeTopBoundary = 21,

  // Right eye, right corner.
  FaceAnnotation_Landmark_Type_RightEyeRightCorner = 22,

  // Right eye, bottom boundary.
  FaceAnnotation_Landmark_Type_RightEyeBottomBoundary = 23,

  // Right eye, left corner.
  FaceAnnotation_Landmark_Type_RightEyeLeftCorner = 24,

  // Left eyebrow, upper midpoint.
  FaceAnnotation_Landmark_Type_LeftEyebrowUpperMidpoint = 25,

  // Right eyebrow, upper midpoint.
  FaceAnnotation_Landmark_Type_RightEyebrowUpperMidpoint = 26,

  // Left ear tragion.
  FaceAnnotation_Landmark_Type_LeftEarTragion = 27,

  // Right ear tragion.
  FaceAnnotation_Landmark_Type_RightEarTragion = 28,

  // Left eye pupil.
  FaceAnnotation_Landmark_Type_LeftEyePupil = 29,

  // Right eye pupil.
  FaceAnnotation_Landmark_Type_RightEyePupil = 30,

  // Forehead glabella.
  FaceAnnotation_Landmark_Type_ForeheadGlabella = 31,

  // Chin gnathion.
  FaceAnnotation_Landmark_Type_ChinGnathion = 32,

  // Chin left gonion.
  FaceAnnotation_Landmark_Type_ChinLeftGonion = 33,

  // Chin right gonion.
  FaceAnnotation_Landmark_Type_ChinRightGonion = 34,
};

GPBEnumDescriptor *FaceAnnotation_Landmark_Type_EnumDescriptor(void);

BOOL FaceAnnotation_Landmark_Type_IsValidValue(int32_t value);

#pragma mark - ImageAnnotatorRoot

@interface ImageAnnotatorRoot : GPBRootObject

// The base class provides:
//   + (GPBExtensionRegistry *)extensionRegistry;
// which is an GPBExtensionRegistry that includes all the extensions defined by
// this file and all files that it depends on.

@end

#pragma mark - Feature

typedef GPB_ENUM(Feature_FieldNumber) {
  Feature_FieldNumber_Type = 1,
  Feature_FieldNumber_MaxResults = 2,
};

// The <em>Feature</em> indicates what type of image detection task to perform.
// Users describe the type of Vision tasks to perform over images by
// using <em>Feature</em>s. Features encode the Vision vertical to operate on
// and the number of top-scoring results to return.
@interface Feature : GPBMessage

// The feature type.
@property(nonatomic, readwrite) Feature_Type type;

// Maximum number of results of this type.
@property(nonatomic, readwrite) int32_t maxResults;

@end

int32_t Feature_Type_RawValue(Feature *message);
void SetFeature_Type_RawValue(Feature *message, int32_t value);

#pragma mark - ImageSource

typedef GPB_ENUM(ImageSource_FieldNumber) {
  ImageSource_FieldNumber_GcsImageUri = 1,
};

// External image source (i.e. Google Cloud Storage image location).
@interface ImageSource : GPBMessage

// Google Cloud Storage image URI. It must be in the following form:
// "gs://bucket_name/object_name". For more
// details, please see: https://cloud.google.com/storage/docs/reference-uris.
// NOTE: Cloud Storage object versioning is not supported!
@property(nonatomic, readwrite, copy, null_resettable) NSString *gcsImageUri;

@end

#pragma mark - Image

typedef GPB_ENUM(Image_FieldNumber) {
  Image_FieldNumber_Content = 1,
  Image_FieldNumber_Source = 2,
};

// Client image to perform Vision tasks over.
@interface Image : GPBMessage

// Image content, represented as a stream of bytes.
@property(nonatomic, readwrite, copy, null_resettable) NSData *content;

// Google Cloud Storage image location. If both 'content' and 'source'
// are filled for an image, 'content' takes precedence and it will be
// used for performing the image annotation request.
@property(nonatomic, readwrite) BOOL hasSource;
@property(nonatomic, readwrite, strong, null_resettable) ImageSource *source;

@end

#pragma mark - FaceAnnotation

typedef GPB_ENUM(FaceAnnotation_FieldNumber) {
  FaceAnnotation_FieldNumber_BoundingPoly = 1,
  FaceAnnotation_FieldNumber_FdBoundingPoly = 2,
  FaceAnnotation_FieldNumber_LandmarksArray = 3,
  FaceAnnotation_FieldNumber_RollAngle = 4,
  FaceAnnotation_FieldNumber_PanAngle = 5,
  FaceAnnotation_FieldNumber_TiltAngle = 6,
  FaceAnnotation_FieldNumber_DetectionConfidence = 7,
  FaceAnnotation_FieldNumber_LandmarkingConfidence = 8,
  FaceAnnotation_FieldNumber_JoyLikelihood = 9,
  FaceAnnotation_FieldNumber_SorrowLikelihood = 10,
  FaceAnnotation_FieldNumber_AngerLikelihood = 11,
  FaceAnnotation_FieldNumber_SurpriseLikelihood = 12,
  FaceAnnotation_FieldNumber_UnderExposedLikelihood = 13,
  FaceAnnotation_FieldNumber_BlurredLikelihood = 14,
  FaceAnnotation_FieldNumber_HeadwearLikelihood = 15,
};

// A face annotation contains the results of face detection.
@interface FaceAnnotation : GPBMessage

// The bounding polygon around the face. The coordinates of the bounding box
// are in the original image's scale, as returned in ImageParams.
// The bounding box is computed to "frame" the face in accordance with human
// expectations. It is based on the landmarker results.
@property(nonatomic, readwrite) BOOL hasBoundingPoly;
@property(nonatomic, readwrite, strong, null_resettable) BoundingPoly *boundingPoly;

// This bounding polygon is tighter than the previous
// <code>boundingPoly</code>, and
// encloses only the skin part of the face. Typically, it is used to
// eliminate the face from any image analysis that detects the
// "amount of skin" visible in an image. It is not based on the
// landmarker results, only on the initial face detection, hence
// the <code>fd</code> (face detection) prefix.
@property(nonatomic, readwrite) BOOL hasFdBoundingPoly;
@property(nonatomic, readwrite, strong, null_resettable) BoundingPoly *fdBoundingPoly;

// Detected face landmarks.
// |landmarksArray| contains |FaceAnnotation_Landmark|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *landmarksArray;
@property(nonatomic, readonly) NSUInteger landmarksArray_Count;

// Roll angle. Indicates the amount of clockwise/anti-clockwise rotation of
// the
// face relative to the image vertical, about the axis perpendicular to the
// face. Range [-180,180].
@property(nonatomic, readwrite) float rollAngle;

// Yaw angle. Indicates the leftward/rightward angle that the face is
// pointing, relative to the vertical plane perpendicular to the image. Range
// [-180,180].
@property(nonatomic, readwrite) float panAngle;

// Pitch angle. Indicates the upwards/downwards angle that the face is
// pointing
// relative to the image's horizontal plane. Range [-180,180].
@property(nonatomic, readwrite) float tiltAngle;

// Detection confidence. Range [0, 1].
@property(nonatomic, readwrite) float detectionConfidence;

// Face landmarking confidence. Range [0, 1].
@property(nonatomic, readwrite) float landmarkingConfidence;

// Joy likelihood.
@property(nonatomic, readwrite) Likelihood joyLikelihood;

// Sorrow likelihood.
@property(nonatomic, readwrite) Likelihood sorrowLikelihood;

// Anger likelihood.
@property(nonatomic, readwrite) Likelihood angerLikelihood;

// Surprise likelihood.
@property(nonatomic, readwrite) Likelihood surpriseLikelihood;

// Under-exposed likelihood.
@property(nonatomic, readwrite) Likelihood underExposedLikelihood;

// Blurred likelihood.
@property(nonatomic, readwrite) Likelihood blurredLikelihood;

// Headwear likelihood.
@property(nonatomic, readwrite) Likelihood headwearLikelihood;

@end

int32_t FaceAnnotation_JoyLikelihood_RawValue(FaceAnnotation *message);
void SetFaceAnnotation_JoyLikelihood_RawValue(FaceAnnotation *message, int32_t value);

int32_t FaceAnnotation_SorrowLikelihood_RawValue(FaceAnnotation *message);
void SetFaceAnnotation_SorrowLikelihood_RawValue(FaceAnnotation *message, int32_t value);

int32_t FaceAnnotation_AngerLikelihood_RawValue(FaceAnnotation *message);
void SetFaceAnnotation_AngerLikelihood_RawValue(FaceAnnotation *message, int32_t value);

int32_t FaceAnnotation_SurpriseLikelihood_RawValue(FaceAnnotation *message);
void SetFaceAnnotation_SurpriseLikelihood_RawValue(FaceAnnotation *message, int32_t value);

int32_t FaceAnnotation_UnderExposedLikelihood_RawValue(FaceAnnotation *message);
void SetFaceAnnotation_UnderExposedLikelihood_RawValue(FaceAnnotation *message, int32_t value);

int32_t FaceAnnotation_BlurredLikelihood_RawValue(FaceAnnotation *message);
void SetFaceAnnotation_BlurredLikelihood_RawValue(FaceAnnotation *message, int32_t value);

int32_t FaceAnnotation_HeadwearLikelihood_RawValue(FaceAnnotation *message);
void SetFaceAnnotation_HeadwearLikelihood_RawValue(FaceAnnotation *message, int32_t value);

#pragma mark - FaceAnnotation_Landmark

typedef GPB_ENUM(FaceAnnotation_Landmark_FieldNumber) {
  FaceAnnotation_Landmark_FieldNumber_Type = 3,
  FaceAnnotation_Landmark_FieldNumber_Position = 4,
};

// A face-specific landmark (for example, a face feature).
@interface FaceAnnotation_Landmark : GPBMessage

// Face landmark type.
@property(nonatomic, readwrite) FaceAnnotation_Landmark_Type type;

// Face landmark position.
@property(nonatomic, readwrite) BOOL hasPosition;
@property(nonatomic, readwrite, strong, null_resettable) Position *position;

@end

int32_t FaceAnnotation_Landmark_Type_RawValue(FaceAnnotation_Landmark *message);
void SetFaceAnnotation_Landmark_Type_RawValue(FaceAnnotation_Landmark *message, int32_t value);

#pragma mark - LocationInfo

typedef GPB_ENUM(LocationInfo_FieldNumber) {
  LocationInfo_FieldNumber_LatLng = 1,
};

// Detected entity location information.
@interface LocationInfo : GPBMessage

// Lat - long location coordinates.
@property(nonatomic, readwrite) BOOL hasLatLng;
@property(nonatomic, readwrite, strong, null_resettable) LatLng *latLng;

@end

#pragma mark - Property

typedef GPB_ENUM(Property_FieldNumber) {
  Property_FieldNumber_Name = 1,
  Property_FieldNumber_Value = 2,
};

// Arbitrary name/value pair.
@interface Property : GPBMessage

// Name of the property.
@property(nonatomic, readwrite, copy, null_resettable) NSString *name;

// Value of the property.
@property(nonatomic, readwrite, copy, null_resettable) NSString *value;

@end

#pragma mark - EntityAnnotation

typedef GPB_ENUM(EntityAnnotation_FieldNumber) {
  EntityAnnotation_FieldNumber_Mid = 1,
  EntityAnnotation_FieldNumber_Locale = 2,
  EntityAnnotation_FieldNumber_Description_p = 3,
  EntityAnnotation_FieldNumber_Score = 4,
  EntityAnnotation_FieldNumber_Confidence = 5,
  EntityAnnotation_FieldNumber_Topicality = 6,
  EntityAnnotation_FieldNumber_BoundingPoly = 7,
  EntityAnnotation_FieldNumber_LocationsArray = 8,
  EntityAnnotation_FieldNumber_PropertiesArray = 9,
};

// Set of detected entity features.
@interface EntityAnnotation : GPBMessage

// Knowledge Graph entity ID. Maps to a freebase entity ID.
// (for example, "Google" maps to: mid /m/045c7b).
@property(nonatomic, readwrite, copy, null_resettable) NSString *mid;

// The language code for the locale in which the entity textual
// <code>description</code> (next field) is expressed.
@property(nonatomic, readwrite, copy, null_resettable) NSString *locale;

// Entity textual description, expressed in its <code>locale</code> language.
@property(nonatomic, readwrite, copy, null_resettable) NSString *description_p;

// Overall score of the result. Range [0, 1].
@property(nonatomic, readwrite) float score;

// The accuracy of the entity recognition in an image.
// For example, for an image containing 'Eiffel Tower,' this field represents
// the confidence that there is a tower in the query image. Range [0, 1].
@property(nonatomic, readwrite) float confidence;

// The relevancy of the ICA (Image Content Annotation) label to the
// image. For example, the relevancy of 'tower' to an image containing
// 'Eiffel Tower' is likely higher than an image containing a distant towering
// building, though the confidence that there is a tower may be the same.
// Range [0, 1].
@property(nonatomic, readwrite) float topicality;

// Image region to which this entity belongs.
@property(nonatomic, readwrite) BOOL hasBoundingPoly;
@property(nonatomic, readwrite, strong, null_resettable) BoundingPoly *boundingPoly;

// The location information for the recognized entity. Multiple
// <code>LocationInfo</code> elements can be present since one location may
// indicate the location of the scene in the query image, and another the
// location of the place where the query image was taken. Location information
// is usually present for landmarks.
// |locationsArray| contains |LocationInfo|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *locationsArray;
@property(nonatomic, readonly) NSUInteger locationsArray_Count;

// Some entities can have additional optional <code>Property</code> fields.
// For example a different kind of score or string that qualifies the entity.
// |propertiesArray| contains |Property|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *propertiesArray;
@property(nonatomic, readonly) NSUInteger propertiesArray_Count;

@end

#pragma mark - SafeSearchAnnotation

typedef GPB_ENUM(SafeSearchAnnotation_FieldNumber) {
  SafeSearchAnnotation_FieldNumber_Adult = 1,
  SafeSearchAnnotation_FieldNumber_Spoof = 2,
  SafeSearchAnnotation_FieldNumber_Medical = 3,
  SafeSearchAnnotation_FieldNumber_Violence = 4,
  SafeSearchAnnotation_FieldNumber_AdultScore = 5,
  SafeSearchAnnotation_FieldNumber_SpoofScore = 6,
  SafeSearchAnnotation_FieldNumber_MedicalScore = 7,
  SafeSearchAnnotation_FieldNumber_ViolenceScore = 8,
};

// Set of features pertaining to the image, computed by various computer vision
// methods over safe-search verticals (for example, adult, spoof, medical,
// violence).
@interface SafeSearchAnnotation : GPBMessage

// Represents the adult contents likelihood for the image.
@property(nonatomic, readwrite) Likelihood adult;

// Spoof likelihood. The likelihood that an obvious modification
// was made to the image's canonical version to make it appear
// funny or offensive.
@property(nonatomic, readwrite) Likelihood spoof;

// Likelihood this is a medical image.
@property(nonatomic, readwrite) Likelihood medical;

// Violence likelihood.
@property(nonatomic, readwrite) Likelihood violence;

// Raw adult score.
@property(nonatomic, readwrite) float adultScore;

// Raw spoof score.
@property(nonatomic, readwrite) float spoofScore;

// Raw medical score.
@property(nonatomic, readwrite) float medicalScore;

// Raw violence score.
@property(nonatomic, readwrite) float violenceScore;

@end

int32_t SafeSearchAnnotation_Adult_RawValue(SafeSearchAnnotation *message);
void SetSafeSearchAnnotation_Adult_RawValue(SafeSearchAnnotation *message, int32_t value);

int32_t SafeSearchAnnotation_Spoof_RawValue(SafeSearchAnnotation *message);
void SetSafeSearchAnnotation_Spoof_RawValue(SafeSearchAnnotation *message, int32_t value);

int32_t SafeSearchAnnotation_Medical_RawValue(SafeSearchAnnotation *message);
void SetSafeSearchAnnotation_Medical_RawValue(SafeSearchAnnotation *message, int32_t value);

int32_t SafeSearchAnnotation_Violence_RawValue(SafeSearchAnnotation *message);
void SetSafeSearchAnnotation_Violence_RawValue(SafeSearchAnnotation *message, int32_t value);

#pragma mark - LatLongRect

typedef GPB_ENUM(LatLongRect_FieldNumber) {
  LatLongRect_FieldNumber_MinLatLng = 1,
  LatLongRect_FieldNumber_MaxLatLng = 2,
};

// Rectangle determined by min and max LatLng pairs.
@interface LatLongRect : GPBMessage

// Min lat/long pair.
@property(nonatomic, readwrite) BOOL hasMinLatLng;
@property(nonatomic, readwrite, strong, null_resettable) LatLng *minLatLng;

// Max lat/long pair.
@property(nonatomic, readwrite) BOOL hasMaxLatLng;
@property(nonatomic, readwrite, strong, null_resettable) LatLng *maxLatLng;

@end

#pragma mark - ColorInfo

typedef GPB_ENUM(ColorInfo_FieldNumber) {
  ColorInfo_FieldNumber_Color = 1,
  ColorInfo_FieldNumber_Score = 2,
  ColorInfo_FieldNumber_PixelFraction = 3,
};

// Color information consists of RGB channels, score and fraction of
// image the color occupies in the image.
@interface ColorInfo : GPBMessage

// RGB components of the color.
@property(nonatomic, readwrite) BOOL hasColor;
@property(nonatomic, readwrite, strong, null_resettable) Color *color;

// Image-specific score for this color. Value in range [0, 1].
@property(nonatomic, readwrite) float score;

// Stores the fraction of pixels the color occupies in the image.
// Value in range [0, 1].
@property(nonatomic, readwrite) float pixelFraction;

@end

#pragma mark - DominantColorsAnnotation

typedef GPB_ENUM(DominantColorsAnnotation_FieldNumber) {
  DominantColorsAnnotation_FieldNumber_ColorsArray = 1,
};

// Set of dominant colors and their corresponding scores.
@interface DominantColorsAnnotation : GPBMessage

// RGB color values, with their score and pixel fraction.
// |colorsArray| contains |ColorInfo|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *colorsArray;
@property(nonatomic, readonly) NSUInteger colorsArray_Count;

@end

#pragma mark - ImageProperties

typedef GPB_ENUM(ImageProperties_FieldNumber) {
  ImageProperties_FieldNumber_DominantColors = 1,
};

// Stores image properties (e.g. dominant colors).
@interface ImageProperties : GPBMessage

// If present, dominant colors completed successfully.
@property(nonatomic, readwrite) BOOL hasDominantColors;
@property(nonatomic, readwrite, strong, null_resettable) DominantColorsAnnotation *dominantColors;

@end

#pragma mark - ImageContext

typedef GPB_ENUM(ImageContext_FieldNumber) {
  ImageContext_FieldNumber_LatLongRect = 1,
  ImageContext_FieldNumber_LanguageHintsArray = 2,
};

// Image context.
@interface ImageContext : GPBMessage

// Lat/long rectangle that specifies the location of the image.
@property(nonatomic, readwrite) BOOL hasLatLongRect;
@property(nonatomic, readwrite, strong, null_resettable) LatLongRect *latLongRect;

// List of languages to use for TEXT_DETECTION. In most cases, an empty value
// will yield the best results as it will allow text detection to
// automatically detect the text language. For languages based on the latin
// alphabet a hint is not needed. In rare cases, when the language of
// the text in the image is known in advance, setting this hint will help get
// better results (although it will hurt a great deal if the hint is wrong).
// Text detection will return an error if one or more of the languages
// specified here are not supported. The exact list of supported languages are
// specified here:
// https://cloud.google.com/translate/v2/using_rest#language-params
// |languageHintsArray| contains |NSString|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *languageHintsArray;
@property(nonatomic, readonly) NSUInteger languageHintsArray_Count;

@end

#pragma mark - AnnotateImageRequest

typedef GPB_ENUM(AnnotateImageRequest_FieldNumber) {
  AnnotateImageRequest_FieldNumber_Image = 1,
  AnnotateImageRequest_FieldNumber_FeaturesArray = 2,
  AnnotateImageRequest_FieldNumber_ImageContext = 3,
};

// Request for performing Vision tasks over a user-provided image, with
// user-requested features.
@interface AnnotateImageRequest : GPBMessage

// The image to be processed.
@property(nonatomic, readwrite) BOOL hasImage;
@property(nonatomic, readwrite, strong, null_resettable) Image *image;

// Requested features.
// |featuresArray| contains |Feature|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *featuresArray;
@property(nonatomic, readonly) NSUInteger featuresArray_Count;

// Additional context that may accompany the image.
@property(nonatomic, readwrite) BOOL hasImageContext;
@property(nonatomic, readwrite, strong, null_resettable) ImageContext *imageContext;

@end

#pragma mark - AnnotateImageResponse

typedef GPB_ENUM(AnnotateImageResponse_FieldNumber) {
  AnnotateImageResponse_FieldNumber_FaceAnnotationsArray = 1,
  AnnotateImageResponse_FieldNumber_LandmarkAnnotationsArray = 2,
  AnnotateImageResponse_FieldNumber_LogoAnnotationsArray = 3,
  AnnotateImageResponse_FieldNumber_LabelAnnotationsArray = 4,
  AnnotateImageResponse_FieldNumber_TextAnnotationsArray = 5,
  AnnotateImageResponse_FieldNumber_SafeSearchAnnotation = 6,
  AnnotateImageResponse_FieldNumber_ImagePropertiesAnnotation = 8,
  AnnotateImageResponse_FieldNumber_Error = 9,
};

// Response to an image annotation request.
@interface AnnotateImageResponse : GPBMessage

// If present, face detection completed successfully.
// |faceAnnotationsArray| contains |FaceAnnotation|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *faceAnnotationsArray;
@property(nonatomic, readonly) NSUInteger faceAnnotationsArray_Count;

// If present, landmark detection completed successfully.
// |landmarkAnnotationsArray| contains |EntityAnnotation|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *landmarkAnnotationsArray;
@property(nonatomic, readonly) NSUInteger landmarkAnnotationsArray_Count;

// If present, logo detection completed successfully.
// |logoAnnotationsArray| contains |EntityAnnotation|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *logoAnnotationsArray;
@property(nonatomic, readonly) NSUInteger logoAnnotationsArray_Count;

// If present, label detection completed successfully.
// |labelAnnotationsArray| contains |EntityAnnotation|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *labelAnnotationsArray;
@property(nonatomic, readonly) NSUInteger labelAnnotationsArray_Count;

// If present, text (OCR) detection completed successfully.
// |textAnnotationsArray| contains |EntityAnnotation|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *textAnnotationsArray;
@property(nonatomic, readonly) NSUInteger textAnnotationsArray_Count;

// If present, safe-search annotation completed successfully.
@property(nonatomic, readwrite) BOOL hasSafeSearchAnnotation;
@property(nonatomic, readwrite, strong, null_resettable) SafeSearchAnnotation *safeSearchAnnotation;

// If present, image properties were extracted successfully.
@property(nonatomic, readwrite) BOOL hasImagePropertiesAnnotation;
@property(nonatomic, readwrite, strong, null_resettable) ImageProperties *imagePropertiesAnnotation;

// If set, represents the error message for the operation.
// Note that filled-in mage annotations are guaranteed to be
// correct, even when <code>error</code> is non-empty.
@property(nonatomic, readwrite) BOOL hasError;
@property(nonatomic, readwrite, strong, null_resettable) Status *error;

@end

#pragma mark - BatchAnnotateImagesRequest

typedef GPB_ENUM(BatchAnnotateImagesRequest_FieldNumber) {
  BatchAnnotateImagesRequest_FieldNumber_RequestsArray = 1,
  BatchAnnotateImagesRequest_FieldNumber_User = 2,
};

// Multiple image annotation requests are batched into a single service call.
@interface BatchAnnotateImagesRequest : GPBMessage

// Individual image annotation requests for this batch.
// |requestsArray| contains |AnnotateImageRequest|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *requestsArray;
@property(nonatomic, readonly) NSUInteger requestsArray_Count;

// User for this batch annotate image request.
@property(nonatomic, readwrite, copy, null_resettable) NSString *user;

@end

#pragma mark - BatchAnnotateImagesResponse

typedef GPB_ENUM(BatchAnnotateImagesResponse_FieldNumber) {
  BatchAnnotateImagesResponse_FieldNumber_ResponsesArray = 1,
};

// Response to a batch image annotation request.
@interface BatchAnnotateImagesResponse : GPBMessage

// Individual responses to image annotation requests within the batch.
// |responsesArray| contains |AnnotateImageResponse|
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray *responsesArray;
@property(nonatomic, readonly) NSUInteger responsesArray_Count;

@end

NS_ASSUME_NONNULL_END

CF_EXTERN_C_END

// @@protoc_insertion_point(global_scope)
